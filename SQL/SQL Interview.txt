1.Explain the difference between INNER JOIN, LEFT JOIN, RIGHT JOIN, and FULL JOIN.
INNER JOIN
Combines rows from two tables where there is a match in the specified condition 
(usually on a foreign key).

SELECT columns
FROM Table1
INNER JOIN Table2
ON Table1.column = Table2.column;


LEFT JOIN
Returns all rows from the left table and the matching rows from the right table. 
If there’s no match, the result will include NULL for columns from the right table.

SELECT columns
FROM Table1
LEFT JOIN Table2
ON Table1.column = Table2.column;


RIGHT JOIN
Returns all rows from the right table and the matching rows from the left table. 
If there’s no match, the result will include NULL for columns from the left table.

SELECT columns
FROM Table1
RIGHT JOIN Table2
ON Table1.column = Table2.column;


FULL JOIN
Combines the result of both LEFT JOIN and RIGHT JOIN. Returns all rows from both tables,
with NULL in place for unmatched rows from either table.

SELECT columns
FROM Table1
FULL JOIN Table2
ON Table1.column = Table2.column;


2.What is a primary key, foreign key, and unique key?

Primary Key
A primary key is a column (or a combination of columns) in a table that uniquely identifies 
each row in the table.

CREATE TABLE Students (
    StudentID INT PRIMARY KEY,
    Name VARCHAR(50),
    Age INT
);


Foreign Key
A foreign key is a column (or a combination of columns) in one table that establishes a
relationship between the data in two tables. It references the primary key in
another table.

CREATE TABLE Students (
    StudentID INT PRIMARY KEY,
    Name VARCHAR(50)
);

CREATE TABLE Courses (
    CourseID INT PRIMARY KEY,
    StudentID INT,
    CourseName VARCHAR(50),
    FOREIGN KEY (StudentID) REFERENCES Students(StudentID)
);


Unique Key
A unique key is a column (or a combination of columns) in a table that ensures all values in the column(s)
are unique.

CREATE TABLE Employees (
    EmployeeID INT PRIMARY KEY,
    Email VARCHAR(50) UNIQUE,
    Name VARCHAR(50)
);


3. What is the difference between WHERE and HAVING clauses?

WHERE Clause
Filters rows before any grouping or aggregation is performed.
Before GROUP BY or aggregation.

Used with non-aggregated columns (columns that are not the result of aggregate functions 
like SUM, AVG, COUNT, etc.)

SELECT *
FROM Orders
WHERE Amount > 400;


HAVING Clause
Filters groups after grouping and aggregation are performed.
After GROUP BY or aggregation.
Can use aggregate functions to filter data, such as SUM, AVG, COUNT.

SELECT CustomerID, SUM(Amount) AS TotalAmount
FROM Orders
GROUP BY CustomerID
HAVING SUM(Amount) > 600;


4. Explain the use of indexes. What are clustered and non-clustered indexes?

Indexes in SQL

An index is a database object used to improve the performance of queries by
allowing the database to find and retrieve data faster. It works similarly to 
an index in a book, which helps you locate information without scanning every page.

Clustered Index
A clustered index determines the physical order of rows in the table. The table data is stored in the same order
as the index.

Non-Clustered Index
A non-clustered index is a separate structure that contains a sorted list of references (pointers) to the rows in the table. 
It does not affect the physical order of the rows in the table.

5. How does the GROUP BY clause work? Can you use it without aggregate functions?

GROUP BY Clause in SQL
The GROUP BY clause is used to group rows that have the same values in specified columns.
It is commonly paired with aggregate functions (e.g., SUM, AVG, COUNT, MAX, MIN)
to perform calculations on each group.

How GROUP BY Works
1.Group Formation:
The GROUP BY clause divides the result set into groups based on the specified column(s).
2.Aggregate Application:
Aggregate functions are applied to each group to return summarized data.
3.Order of Execution:

FROM → WHERE → GROUP BY → HAVING → SELECT → ORDER BY
The GROUP BY clause processes the rows after the WHERE clause filters the data.

Using GROUP BY with Aggregate Functions
A typical use case for GROUP BY involves aggregate functions:


Can You Use GROUP BY Without Aggregate Functions?
Yes, the GROUP BY clause can be used without aggregate functions. In such cases, it simply groups rows based 
on the specified columns, and only one row is returned for each unique group.


6. What is a CTE (Common Table Expression), and when should you use it?
A CTE (Common Table Expression) is a temporary, named result set defined within a WITH clause that can be 
referenced in a SQL query. It is similar to a derived table or a subquery but 
offers improved readability and reusability.

1. Simplifying Complex Queries
Purpose: Divide a large, complex query into smaller, more manageable parts.

WITH AvgSalaries AS (
    SELECT DepartmentID, AVG(Salary) AS AvgSalary
    FROM Employees
    GROUP BY DepartmentID
)
SELECT e.EmployeeID, e.Name, e.Salary, a.AvgSalary
FROM Employees e
JOIN AvgSalaries a ON e.DepartmentID = a.DepartmentID
WHERE e.Salary > a.AvgSalary;

2. Recursive Queries
Purpose: Solve hierarchical or recursive problems like organizational charts or tree structures.

WITH EmployeeHierarchy AS (
    SELECT EmployeeID, ManagerID, Name
    FROM Employees
    WHERE ManagerID IS NULL -- Top-level manager

    UNION ALL

    SELECT e.EmployeeID, e.ManagerID, e.Name
    FROM Employees e
    INNER JOIN EmployeeHierarchy eh ON e.ManagerID = eh.EmployeeID
)
SELECT * FROM EmployeeHierarchy;

3. Improving Query Readability

Purpose: Create a named, reusable query result set instead of nesting
multiple subqueries.

SELECT DepartmentID, EmployeeID, Salary
FROM Employees
WHERE Salary > (SELECT AVG(Salary) FROM Employees);

4. Reusability Within a Query

Purpose: Reference the same intermediate result multiple times in a query.

WITH DeptSalary AS (
    SELECT DepartmentID, SUM(Salary) AS TotalSalary, AVG(Salary) AS AvgSalary
    FROM Employees
    GROUP BY DepartmentID
)
SELECT DepartmentID, TotalSalary, AvgSalary
FROM DeptSalary
WHERE TotalSalary > 50000;


What is a CTE (Common Table Expression)?
A CTE (Common Table Expression) is a temporary, named result set defined within a WITH clause that can be referenced in a SQL query. It is similar to a derived table or a subquery but offers improved readability and reusability.

Syntax:
sql
Copy code
WITH CTE_Name (Column1, Column2, ...)
AS (
    SELECT Columns
    FROM SomeTable
    WHERE Condition
)
SELECT Columns
FROM CTE_Name
WHERE AnotherCondition;
When to Use CTEs
CTEs are useful in various scenarios where they improve code readability, reusability, or help solve complex queries. Common use cases include:

1. Simplifying Complex Queries
Purpose: Divide a large, complex query into smaller, more manageable parts.
Example: Find all employees earning above the average salary in their department:
sql
Copy code
WITH AvgSalaries AS (
    SELECT DepartmentID, AVG(Salary) AS AvgSalary
    FROM Employees
    GROUP BY DepartmentID
)
SELECT e.EmployeeID, e.Name, e.Salary, a.AvgSalary
FROM Employees e
JOIN AvgSalaries a ON e.DepartmentID = a.DepartmentID
WHERE e.Salary > a.AvgSalary;
2. Recursive Queries
Purpose: Solve hierarchical or recursive problems like organizational charts or tree structures.
Example: Find all employees in a hierarchy under a specific manager:
sql
Copy code
WITH EmployeeHierarchy AS (
    SELECT EmployeeID, ManagerID, Name
    FROM Employees
    WHERE ManagerID IS NULL -- Top-level manager

    UNION ALL

    SELECT e.EmployeeID, e.ManagerID, e.Name
    FROM Employees e
    INNER JOIN EmployeeHierarchy eh ON e.ManagerID = eh.EmployeeID
)
SELECT * FROM EmployeeHierarchy;
3. Improving Query Readability
Purpose: Create a named, reusable query result set instead of nesting multiple subqueries.
Example: Without a CTE (nested subquery):
sql
Copy code
SELECT DepartmentID, EmployeeID, Salary
FROM Employees
WHERE Salary > (SELECT AVG(Salary) FROM Employees);
Using a CTE:
sql
Copy code
WITH AvgSalary AS (
    SELECT AVG(Salary) AS AvgSalary FROM Employees
)
SELECT DepartmentID, EmployeeID, Salary
FROM Employees, AvgSalary
WHERE Salary > AvgSalary.AvgSalary;
4. Reusability Within a Query
Purpose: Reference the same intermediate result multiple times in a query.
Example: Find the total salary and average salary by department:
sql
Copy code
WITH DeptSalary AS (
    SELECT DepartmentID, SUM(Salary) AS TotalSalary, AVG(Salary) AS AvgSalary
    FROM Employees
    GROUP BY DepartmentID
)
SELECT DepartmentID, TotalSalary, AvgSalary
FROM DeptSalary
WHERE TotalSalary > 50000;


5. Simplifying Aggregates and Window Functions

Purpose: Simplify calculations involving aggregate functions or window functions.

WITH DepartmentStats AS (
    SELECT DepartmentID, AVG(Salary) AS AvgSalary
    FROM Employees
    GROUP BY DepartmentID
)
SELECT e.EmployeeID, e.Name, e.Salary, ds.AvgSalary
FROM Employees e
JOIN DepartmentStats ds ON e.DepartmentID = ds.DepartmentID
WHERE e.Salary > ds.AvgSalary;



7. Explain window functions. What is the difference between ROW_NUMBER(), RANK(), and DENSE_RANK()?
Window Functions in SQL
Window functions perform calculations across a set of rows that are related to the current row in a query.
Unlike aggregate functions, which collapse rows into a single result, window functions
preserve the individual rows while adding calculated data.

Key Components of Window Functions
OVER Clause:

1.Defines the "window" of rows the function operates on.
You can specify:
Partitioning with PARTITION BY: Divides the data into groups (similar to GROUP BY).
Ordering with ORDER BY: Determines the row order within each partition.

2.Common Window Functions:

Ranking: ROW_NUMBER(), RANK(), DENSE_RANK(), NTILE()
Aggregate: SUM(), AVG(), COUNT(), MAX(), MIN()
Offsets: LEAD(), LAG(), FIRST_VALUE(), LAST_VALUE()

Difference Between ROW_NUMBER(), RANK(), and DENSE_RANK()

All three functions assign a rank to each row in a result 
set based on the specified ORDER BY clause. However, they handle ties differently.

1. ROW_NUMBER()
Assigns a unique number to each row in the result set.
No ties are allowed; even rows with identical values in the ORDER BY clause receive 
distinct ranks.
Use Case: Generating a unique row ID or pagination.

SELECT EmployeeID, Salary,
       ROW_NUMBER() OVER (ORDER BY Salary DESC) AS RowNumber
FROM Employees;


2. RANK()
Assigns a rank to each row, with gaps in the ranking if there are ties.
Rows with identical values in the ORDER BY clause receive the same rank.
The next rank skips based on the number of tied rows.
Use Case: Showing ranks in competitions or performance evaluations.

SELECT EmployeeID, Salary,
       RANK() OVER (ORDER BY Salary DESC) AS Rank
FROM Employees;


3. DENSE_RANK()
Similar to RANK(), but no gaps in the ranking.
Rows with identical values in the ORDER BY clause receive the same rank, but the next
rank is sequential.
Use Case: Grouping ranks closely without gaps.

SELECT EmployeeID, Salary,
       DENSE_RANK() OVER (ORDER BY Salary DESC) AS DenseRank
FROM Employees;


When to Use ROW_NUMBER(), RANK(), and DENSE_RANK()?
ROW_NUMBER(): When you need unique sequential numbering for rows, like pagination or generating IDs.
RANK(): When you need to rank items with ties, but you want gaps in the rank sequence (e.g., competition rankings).
DENSE_RANK(): When you need to rank items with ties but want continuous ranking
without gaps.

8. What is a correlated subquery? How is it different from a regular subquery?

What is a Correlated Subquery
A correlated subquery is a type of subquery that depends on the outer query for its 
values. For each row processed by the outer query, the subquery is executed, making
the correlated subquery row-dependent.

In simpler terms:

The inner query (subquery) is evaluated once per row of the outer query.
The subquery refers to a column from the outer query, creating a dependency.


What is a Regular Subquery
A regular subquery (or simple subquery) is independent of the outer query.
It is executed once and its result is used by the outer query.

Key Features of a Regular Subquery:
No dependency on the outer query.
The result of the subquery is static for the outer query.
Commonly used with operators like IN, EXISTS, or as part of a JOIN.

When to Use Correlated vs. Regular Subqueries
Correlated Subqueries: Use when the subquery's result depends on the current row of
the outer query (e.g., comparisons within groups).
Regular Subqueries: Use when the subquery result is static or can be reused across
all rows of the outer query.


9. How can you optimize a query in SQL? List some best practices.
1. Use Indexes Wisely
Indexes help speed up data retrieval by reducing the number of rows scanned.

Clustered Index: Automatically created on the primary key, sorts the physical data
in the table.
Non-Clustered Index: Useful for frequently queried columns.
Best Practices:
Index columns that are frequently used in WHERE, JOIN, GROUP BY, or ORDER BY.
Avoid indexing columns with low selectivity (e.g., Boolean or gender columns).
Regularly maintain indexes by rebuilding or reorganizing them.

2. Avoid SELECT *** (Use Only Required Columns)
Fetching all columns (SELECT *) retrieves unnecessary data and increases I/O
operations.

SELECT * FROM Employees;(Bad)
SELECT EmployeeID, Name, DepartmentID FROM Employees;(Good)

3. Use WHERE Instead of HAVING
The WHERE clause filters rows before grouping, while HAVING filters after. 
Use WHERE for non-aggregated filtering.

SELECT DepartmentID, AVG(Salary)
FROM Employees
WHERE DepartmentID = 5
GROUP BY DepartmentID;


4. Optimize JOINS
Use appropriate JOIN types (INNER JOIN, LEFT JOIN, etc.) based on your requirements.
Ensure that columns used in JOIN conditions are indexed.
Avoid unnecessary joins by removing irrelevant tables.

5. Avoid Using Functions in WHERE Clauses
Using functions on columns prevents indexes from being used, leading to full table
scans.

SELECT * FROM Employees
WHERE HireDate >= '2023-01-01' AND HireDate < '2024-01-01';


6. Use EXISTS Instead of IN for Subqueries
For large data sets, EXISTS is often faster than IN because it stops searching after finding 
the first match.

SELECT EmployeeID, Name
FROM Employees E
WHERE EXISTS (
    SELECT 1
    FROM Departments D
    WHERE D.DepartmentID = E.DepartmentID AND D.Location = 'New York'
);


7. Avoid Correlated Subqueries When Possible
Correlated subqueries execute for every row, which can be expensive. Use JOIN or 
WITH (CTE) instead.

WITH DepartmentSalary AS (
    SELECT DepartmentID, AVG(Salary) AS AvgSalary
    FROM Employees
    GROUP BY DepartmentID
)
SELECT E.EmployeeID, E.Name
FROM Employees E
INNER JOIN DepartmentSalary DS
ON E.DepartmentID = DS.DepartmentID
WHERE E.Salary > DS.AvgSalary;


8. Use LIMIT or TOP for Large Result Sets
When you only need a subset of data, use LIMIT (MySQL/PostgreSQL)
or TOP (SQL Server) to reduce unnecessary processing.

SELECT * FROM Employees
ORDER BY Salary DESC
LIMIT 10; 

SELECT TOP 10 * FROM Employees
ORDER BY Salary DESC;  

9. Analyze and Optimize Execution Plans
Use tools like EXPLAIN, EXPLAIN PLAN, or SHOWPLAN to analyze how your query is 
executed. Look for:

Full table scans (bad).
Proper use of indexes (good).
Expensive operations like nested loops or sorts.


10. Use Proper Data Types
Use appropriate data types for columns to minimize storage and processing overhead.
E.g., Use INT instead of BIGINT when possible.
Use CHAR for fixed-length data and VARCHAR for variable-length data.
Avoid implicit conversions (e.g., comparing INT with VARCHAR).

11. Normalize or Denormalize as Needed
Normalization: Reduce data redundancy for better storage efficiency and consistency.
Denormalization: Reduce joins in read-heavy systems by combining tables when 
performance is a priority.

12. Avoid OR in WHERE Clauses (Use UNION)
Using OR often leads to full table scans. Replace it with UNION for 
better performance.

SELECT * FROM Employees
WHERE DepartmentID = 1
UNION
SELECT * FROM Employees
WHERE DepartmentID = 2;


13. Use Partitioning for Large Tables
Partition large tables into smaller, manageable segments based on columns like 
date or region. This reduces the amount of data scanned during queries.

CREATE TABLE Orders (
    OrderID INT,
    OrderDate DATE,
    CustomerID INT
)
PARTITION BY RANGE (OrderDate) (
    PARTITION p0 VALUES LESS THAN ('2023-01-01'),
    PARTITION p1 VALUES LESS THAN ('2024-01-01')
);

14. Cache Repeated Queries
If the same query is executed repeatedly, caching its result
can improve performance (at the application level or with database query caching).

15. Use Proper Transactions
Keep transactions short to avoid locking issues.
Use BEGIN TRANSACTION, COMMIT, and ROLLBACK carefully.

16. Update Statistics and Rebuild Indexes
Keep database statistics up to date so the query optimizer can make informed
decisions. Periodically rebuild indexes to reduce fragmentation.

17. Avoid DISTINCT and UNION Unless Necessary
These operations require sorting and deduplication, 
which can be costly. Use them only when duplicates are genuinely a concern.


10. What are database normalization and denormalization? Explain their trade-offs.

Database Normalization
Normalization is a process in database design used to organize data efficiently by
eliminating redundancy and ensuring data integrity. It involves breaking down a table
into smaller, related tables and defining relationships between them.

Forms of Normalization
Normalization follows a set of rules known as normal forms. The common normal forms
are:

First Normal Form (1NF):
Ensures that all columns contain atomic (indivisible) values, and each row is unique.

Second Normal Form (2NF):
Achieved when the table is in 1NF and all non-key attributes depend fully on the primary key.
Example: If a table has a composite primary key, ensure all columns depend on the entire key, 
not just part of it.

Third Normal Form (3NF):
Ensures that the table is in 2NF and there are no transitive dependencies 
(non-key attributes must not depend on other non-key attributes).

Boyce-Codd Normal Form (BCNF):
A stricter version of 3NF where every determinant is a candidate key.

11. How does indexing improve query performance? What are the downsides of having too
many indexes?

How Indexing Improves Query Performance
Indexes in SQL improve query performance by enabling the database engine to quickly
locate and retrieve data without scanning the entire table. They function like a
table of contents in a book, pointing to the exact location of data.

Benefits of Indexing:
1.Faster Query Execution:

Indexes reduce the number of rows the database needs to scan, leading to quicker data
retrieval.
Example:
A query like SELECT * FROM Employees WHERE Name = 'John' will be much faster if the
Name column has an index.

2.Efficient Sorting:

Indexes optimize queries with ORDER BY or GROUP BY clauses by sorting data beforehand.

3.Speed Up Joins:

Indexes on foreign keys or join columns improve the performance of queries involving joins
between tables.

4.Optimized Aggregate Functions:

Queries with aggregate functions like COUNT, SUM, or AVG can perform better when indexes
are used.

5.Facilitates Quick Searching:

Full-text indexes improve searches for text-based data.

Types of Indexes:

1.Clustered Index:

Determines the physical order of rows in a table. Each table can have only one 
clustered index.
Example: A primary key column often has a clustered index.

2.Non-clustered Index:

Points to the actual data in the table without altering its physical order. 
You can have multiple non-clustered indexes on a table.
Example: Creating a non-clustered index on Email for quick lookup.

3.Unique Index:

Ensures all values in a column are unique. It’s often used on primary or 
unique keys.


4.Full-Text Index:

Designed for searching large text-based columns efficiently.

Downsides of Having Too Many Indexes
While indexes improve query performance, too many indexes can lead to several issues:

1.Increased Storage Usage:

Indexes consume additional disk space. For large tables with multiple indexes, 
this can be significant.

2.Slower Write Operations:

Inserts, Updates, and Deletes require updating all associated indexes, 
which can slow down write-heavy workloads.

3.Maintenance Overhead:

Indexes must be kept up to date, requiring additional resources for maintenance 
during operations like UPDATE or INSERT.

4.Query Optimization Issues:

The database optimizer may take longer to decide which index to use for a query, 
especially if there are too many indexes.

5.Index Fragmentation:

Over time, indexes can become fragmented, leading to performance degradation.
Regular index rebuilding or reorganizing is needed to fix this.

6.Diminished Returns:

Adding indexes beyond a certain number may not yield significant performance 
benefits and might slow down queries if the wrong index is chosen.

12. What is the difference between EXISTS and IN? When should you use each?

Difference Between EXISTS and IN
Both EXISTS and IN are used in SQL to filter records based on conditions in a
subquery, but they work differently in terms of how they evaluate the condition
and their performance characteristics.

1.EXISTS

How It Works:

EXISTS checks for the presence of rows in a subquery. It returns TRUE if the 
subquery returns any rows, and FALSE otherwise.
It stops processing as soon as it finds a matching row (short-circuiting).

2.IN
How It Works:

IN checks if a value exists within a static list or the result of a subquery.
It compares the value from the outer query against all the values returned by the 
subquery.

13. How can you identify and resolve deadlocks in a database?

Deadlocks in a Database
A deadlock occurs in a database when two or more processes hold resources and wait 
for other processes to release resources, creating a circular dependency that prevents
all processes from proceeding. This leads to a situation where none of the processes
can continue, effectively halting execution.

14. Explain execution plans and how to interpret them.

An execution plan is a visual representation of the steps and strategies the SQL
database engine uses to execute a query. It shows how the query optimizer chooses
to retrieve the required data, including operations like table scans, index usage,
joins, and sorting.

Execution plans are essential for understanding query performance and optimizing SQL 
queries by identifying inefficiencies like full table scans or poor index usage.

Types of Execution Plans
1.Estimated Execution Plan:

Displays the plan without actually executing the query.
Useful for understanding how the query will behave without affecting the database.

2.Actual Execution Plan:

Provides the execution steps after running the query.
Includes runtime statistics like the number of rows processed at each step.

3.Live Query Statistics:

A real-time visualization of query execution (available in modern SQL tools like
SQL Server Management Studio).

SET STATISTICS TIME, IO ON;
SELECT * FROM Employees;

15. How do transactions work in SQL? What are ACID properties?

What are Transactions in SQL?
A transaction in SQL is a sequence of operations performed as a single logical unit
of work. A transaction ensures that all operations within it either complete
successfully or fail altogether. This guarantees data integrity and consistency.

Transactions typically involve operations like INSERT, UPDATE, DELETE, or SELECT,
and they are managed using commands such as:

BEGIN TRANSACTION: Starts a transaction.
COMMIT: Saves the changes made during the transaction permanently.
ROLLBACK: Reverts the changes made during the transaction if any issue occurs.

ACID Properties
ACID is a set of principles that ensure reliable and consistent database
transactions. Each transaction in SQL should adhere to the following properties:

1.Atomicity:
Ensures that all operations in a transaction are treated as a single unit.
Either all the operations are performed, or none are.
If any step fails, the entire transaction is rolled back.

2.Consistency:
Ensures the database transitions from one valid state to another valid state after
a transaction.
The rules and constraints of the database (e.g., primary key, foreign key, data type)
are preserved.

3.Isolation:
Ensures that multiple transactions occurring concurrently do not interfere with each other.
Transactions must be executed in such a way as if they were running sequentially, even when 
executed in parallel.

4.Durability:
Ensures that once a transaction is committed, its changes are permanent, even in the 
event of a system crash.
Changes are written to persistent storage.

16. Write a query to find the second highest salary in a table.

Using DISTINCT with ORDER BY and LIMIT/OFFSET (SQL Server)
SELECT DISTINCT Salary
FROM Employees
ORDER BY Salary DESC
LIMIT 1 OFFSET 1;


Using TOP and DISTINCT (SQL Server)
SELECT TOP 1 Salary
FROM (
    SELECT DISTINCT Salary
    FROM Employees
    ORDER BY Salary DESC
) AS Temp
ORDER BY Salary;

Using SUBQUERY
SELECT MAX(Salary) AS SecondHighestSalary
FROM Employees
WHERE Salary < (SELECT MAX(Salary) FROM Employees);


Using DENSE_RANK() (SQL Server)
SELECT Salary
FROM (
    SELECT Salary, DENSE_RANK() OVER (ORDER BY Salary DESC) AS Rank
    FROM Employees
) AS Ranked
WHERE Rank = 2;


17. How would you design a schema for an e-commerce platform with products, users, and orders?

1. Users Table
CREATE TABLE Users (
    UserId INT PRIMARY KEY IDENTITY(1,1),
    Name NVARCHAR(100) NOT NULL,         
    Email NVARCHAR(255) UNIQUE NOT NULL, 
    PasswordHash NVARCHAR(255) NOT NULL, 
    PhoneNumber NVARCHAR(15),            
    Address NVARCHAR(500),               
    CreatedAt DATETIME DEFAULT GETDATE(),
    IsAdmin BIT DEFAULT 0                
);

2. Products Table
CREATE TABLE Products (
    ProductId INT PRIMARY KEY IDENTITY(1,1), 
    ProductName NVARCHAR(255) NOT NULL,      
    Description NVARCHAR(MAX),               
    Price DECIMAL(10,2) NOT NULL,            
    Quantity INT NOT NULL DEFAULT 0,         
    Category NVARCHAR(100),                  
    CreatedAt DATETIME DEFAULT GETDATE()   
);

3. Orders Table
CREATE TABLE Orders (
    OrderId INT PRIMARY KEY IDENTITY(1,1),      
    UserId INT NOT NULL,                        
    TotalAmount DECIMAL(10,2) NOT NULL,         
    OrderStatus NVARCHAR(50) DEFAULT 'Pending',
    OrderDate DATETIME DEFAULT GETDATE(),      
    FOREIGN KEY (UserId) REFERENCES Users(UserId)
);

4. OrderDetails Table
CREATE TABLE OrderDetails (
    OrderDetailId INT PRIMARY KEY IDENTITY(1,1), 
    OrderId INT NOT NULL,                        
    ProductId INT NOT NULL,                      
    Quantity INT NOT NULL,                      
    UnitPrice DECIMAL(10,2) NOT NULL,           
    TotalPrice AS (Quantity * UnitPrice) PERSISTED, 
    FOREIGN KEY (OrderId) REFERENCES Orders(OrderId),
    FOREIGN KEY (ProductId) REFERENCES Products(ProductId)
);

5. Categories Table (Optional)
CREATE TABLE Categories (
    CategoryId INT PRIMARY KEY IDENTITY(1,1),   -- Unique identifier for categories
    CategoryName NVARCHAR(100) NOT NULL UNIQUE -- Name of the category
);
ALTER TABLE Products
ADD CategoryId INT,
    FOREIGN KEY (CategoryId) REFERENCES Categories(CategoryId);

6. Payment Table (Optional)
CREATE TABLE Payments (
    PaymentId INT PRIMARY KEY IDENTITY(1,1),   
    OrderId INT NOT NULL,                      
    PaymentMethod NVARCHAR(50) NOT NULL,        
    PaymentStatus NVARCHAR(50) DEFAULT 'Pending',
    PaymentDate DATETIME DEFAULT GETDATE(),     
    FOREIGN KEY (OrderId) REFERENCES Orders(OrderId)
);

7. Reviews Table (Optional)
CREATE TABLE Reviews (
    ReviewId INT PRIMARY KEY IDENTITY(1,1), 
    ProductId INT NOT NULL,                 
    UserId INT NOT NULL,                  
    Rating INT CHECK (Rating BETWEEN 1 AND 5),
    ReviewText NVARCHAR(MAX),         
    ReviewDate DATETIME DEFAULT GETDATE(), 
    FOREIGN KEY (ProductId) REFERENCES Products(ProductId),
    FOREIGN KEY (UserId) REFERENCES Users(UserId)
);


18. How can you retrieve duplicate records from a table?

To retrieve duplicate records from a table in SQL, you can use the GROUP BY clause
with an aggregate function like COUNT(). Duplicates are identified when a column 
(or combination of columns) has more than one occurrence.

SELECT Column1, Column2, ..., COUNT(*)
FROM TableName
GROUP BY Column1, Column2, ...
HAVING COUNT(*) > 1;

19. Write a query to calculate a running total or cumulative sum of a column.
To calculate a running total or cumulative sum of a column in SQL, you can use window
functions like SUM() with the OVER clause.

SELECT 
    Column1, 
    Column2, 
    SUM(TargetColumn) OVER (ORDER BY SortColumn) AS RunningTotal
FROM TableName;

SELECT 
    SaleId, 
    SaleDate, 
    Amount, 
    SUM(Amount) OVER (ORDER BY SaleDate) AS RunningTotal
FROM Sales;

20. How would you handle data migration from one database to another?

Handling data migration from one database to another involves several critical steps
to ensure the data is transferred correctly, securely, and efficiently. Below is a
comprehensive approach for handling this process:

1. Planning and Preparation
Understand the Source and Target Databases: Assess the differences and similarities 
between the source and target databases. This includes understanding the schema, 
data types, constraints, and indexes.
Identify the Scope of Migration: Determine which tables, views, stored procedures,
and other database objects need to be migrated.
Establish Migration Requirements: Understand if the migration is full or 
incremental, the timeline for migration, and any downtime required.

2. Data Mapping
Schema Mapping: Identify how the source database schema maps to the target schema.
This includes comparing table structures, column data types, and relationships.
Data Transformation: Ensure that any differences in data types, formats, or encoding
between the source and target databases are handled correctly. If necessary, write 
transformation rules to modify the data as it moves from the source to the target.

3. Choosing the Migration Approach
Manual Migration: Useful for small databases or migrations involving minor changes. 
Involves exporting and importing data manually using SQL queries or database 
management tools.
Automated Data Migration Tools: Use tools like SQL Server Integration Services (SSIS),
AWS Database Migration Service (DMS), or third-party ETL (Extract, Transform, Load) 
tools for more complex or larger databases.
Backup and Restore: If the source and target databases are of the same type
(e.g., SQL Server to SQL Server), you can back up the source database and restore it
on the target system.
Database Replication: If real-time data migration is required, database replication 
technologies can be used to copy data continuously from the source to the target 
system.

4. Data Extraction
Export Data: Extract data from the source database in a suitable format 
(e.g., CSV, SQL dump, or using an API). For larger datasets, consider breaking the 
extraction into smaller batches to prevent performance issues.
Incremental Extraction: If the data in the source database is constantly changing,
use incremental extraction methods (e.g., Change Data Capture or CDC) to only migrate
the changes.

5. Data Transformation (If Needed)
Data Cleaning: During the transformation process, clean the data to ensure it's free
of errors, inconsistencies, or duplicates.
Format Conversion: If the source and target databases use different formats or data 
types (e.g., different date formats or data type sizes), convert the data accordingly.

6. Data Loading
Batch Loading: For large datasets, consider batch loading to minimize lock contention
and reduce migration time.
Load Data into Target Database: Use an appropriate tool or SQL queries to insert data
into the target database. If necessary, maintain transactional integrity 
(e.g., using BEGIN TRANSACTION and COMMIT) to ensure data consistency during loading.

7. Validation and Testing
Data Validation: Compare the data in the source and target databases to ensure it has
been migrated correctly. This can be done by running validation queries to check
record counts, sums, or checksums for data integrity.

Application Testing: Ensure that the target database works with existing applications.
Run performance tests and verify that the migrated data is accessible and consistent 
with expected results.

Data Reconciliation: Perform detailed data checks and compare sample rows between 
the source and target to ensure all data has been migrated correctly.
8. Handling Downtime and Cutover
Minimize Downtime: If the database migration requires downtime, plan the migration 
during off-peak hours to minimize disruption. Use a rollback plan if necessary.

Cutover: Once the data is successfully loaded and validated in the target system,
perform the cutover by switching the applications to the new database.
9. Post-Migration Tasks
Performance Tuning: Optimize the performance of the target database by rebuilding 
indexes, updating statistics, and reviewing query execution plans.
Backup and Recovery: Take a fresh backup of the target database after the migration
to ensure that you have a fallback option if something goes wrong.

Monitor the Target System: Continuously monitor the performance of the target system
after migration to ensure that it is functioning as expected.

Data Archival: Archive the source database if needed, but ensure it's still accessible
if you need to reference or restore data post-migration.
10. Tools for Data Migration
ETL Tools: Tools like Apache Nifi, Talend, or Informatica are used to extract,
transform, and load data.
Database-Specific Tools: Many database systems provide native migration tools, 
such as:
SQL Server: SQL Server Integration Services (SSIS), Data Migration Assistant (DMA)

MySQL: MySQL Workbench, mysqldump, and mysqlimport
Oracle: Oracle Data Pump, Oracle GoldenGate
PostgreSQL: pg_dump, pg_restore, and pg_dumpall
AWS: AWS Database Migration Service (DMS)
11. Handling Special Cases
Foreign Key Constraints: Disable foreign key constraints temporarily during the data
loading process if necessary, and re-enable them after the migration.
Data Syncing: For real-time systems, set up mechanisms like data replication or 
change data capture (CDC) to synchronize data between source and target during 
migration.

21. Explain stored procedures, triggers, and functions. When would you use each?

1. Stored Procedures
A stored procedure is a precompiled collection of SQL statements that is stored in
the database and can be executed as a single unit. It allows you to encapsulate 
business logic, reduce code duplication, and improve performance by minimizing round
trips between the application and the database.

Key Characteristics of Stored Procedures:
Accept parameters (both input and output).

Allow procedural logic, such as loops and conditionals.

Execute complex queries or a series of SQL statements.

Can be reused across multiple applications or parts of an application.

Precompiled, meaning the database server caches the execution plan, making
them faster.

2. Triggers
A trigger is a special type of stored procedure that automatically executes in 
response to specific events in the database, such as INSERT, UPDATE, or DELETE 
operations on a table. Triggers are used to enforce rules, maintain audit logs, 
or automatically update related tables.

Key Characteristics of Triggers:

Automatically executed when a specified event occurs.

Associated with a specific table or view.

Can enforce referential integrity or maintain history/audit logs.

Cannot be explicitly called like stored procedures or functions.

3. Functions
A function is a database object that performs a specific task and returns a value. Functions are generally used for calculations, formatting, or retrieving scalar or tabular results. Unlike stored procedures, functions cannot perform transactions or modify database data.

Key Characteristics of Functions:

Always return a value (scalar value or table).

Cannot modify data (e.g., no INSERT, UPDATE, or DELETE inside a function).

Can be used in SQL queries (e.g., SELECT, WHERE, or JOIN clauses).

Provide modularity and code reuse for computations or transformations.

22. How would you implement pagination in SQL?

Pagination in SQL is used to retrieve a subset of records from a large dataset
in chunks (or pages), typically for displaying data in a UI with a limited number
of rows per page. Here's how you can implement pagination in SQL using different
approaches:

1. Using OFFSET and FETCH (SQL Server)
The OFFSET specifies how many rows to skip, and FETCH NEXT specifies how many rows to return.

SELECT columns
FROM table
ORDER BY column_name
OFFSET @Offset ROWS
FETCH NEXT @PageSize ROWS ONLY;

2. Using ROW_NUMBER() for Older SQL Versions
You can assign a unique row number to each record using the ROW_NUMBER() 
window function and filter records based on the row numbers.

WITH PaginatedData AS (
    SELECT columns,
           ROW_NUMBER() OVER (ORDER BY column_name) AS RowNum
    FROM table
)
SELECT *
FROM PaginatedData
WHERE RowNum BETWEEN @Start AND @End;

3. Using TOP with a Subquery (SQL Server)

For older versions of SQL Server, you can use a subquery to skip rows and return
the next set.

SELECT TOP (@PageSize) columns
FROM table
WHERE column_name NOT IN (
    SELECT TOP (@Offset) column_name
    FROM table
    ORDER BY column_name
)
ORDER BY column_name;


4. Using LIMIT (MySQL, PostgreSQL)
The LIMIT keyword is often used for pagination. It specifies how many rows to 
return, and OFFSET specifies where to start.

SELECT columns
FROM table
ORDER BY column_name
LIMIT @PageSize OFFSET @Offset;

23. What are the differences between SQL and NoSQL databases? When would you
prefer one over the other?

Differences Between SQL and NoSQL Databases

Aspect               SQL Database                                 NoSQL Database

Data Model		Relational (tables with rows and columns)  Non-relational (document, key-value, graph, 
																	column-family models)
		
Schema               Fixed schema (predefined structure)   Schema-less or dynamic schema

Query Language			Structured Query Language (SQL)		Varies by database (e.g., MongoDB uses BSON, Cassandra uses CQL)
			

Scalability 		Vertically scalable (add more resources		Horizontally scalable (add more servers/nodes)
					to a single server)
					
Data Integrity      Strong ACID compliance (Atomicity, Consistency, 	Eventual consistency with BASE (Basically Available,
																		Soft-state, Eventual consistency)
					Isolation, Durability)	
			
Use Case   			Best for structured, complex queries, and           Best for unstructured or semi-structured data,
																		large-scale applications
					relationships between data
					
					
Performance  		Optimized for complex joins and aggregations       Optimized for high-speed reads and writes


When to Use SQL Databases

1.Structured Data:

Your data is highly structured and requires predefined schemas
(e.g., financial systems, ERP, HR systems).

2.Data Integrity:

You need strong ACID compliance to ensure data accuracy and consistency 
(e.g., banking systems, transactions).

3.Complex Queries:

You need to perform complex joins, aggregations, and analysis on relational data.

4.Small to Medium-Scale Applications:

You’re building applications with moderate data sizes where vertical 
scaling suffices.

5.Example Use Cases:

Banking systems
Inventory management
eCommerce systems (with strict relationships between users, orders, and products)


When to Use NoSQL Databases

1.Unstructured or Semi-Structured Data:

You’re dealing with data that doesn’t fit neatly into tables (e.g., JSON, XML, multimedia, logs).

2.Scalability:

You need horizontal scalability to handle massive amounts of data (e.g., distributed systems, IoT).

3.High-Speed Reads and Writes:

Your application requires low-latency and high-performance (e.g., real-time 
analytics, gaming leaderboards).

4.Dynamic or Evolving Schema:

Your data structure changes frequently or is unpredictable.

5.Eventual Consistency is Acceptable:

Applications can tolerate some delay in data consistency for higher availability 
and partition tolerance.
Example Use Cases:

Social media platforms
Content management systems
IoT applications
Large-scale analytics and reporting

SQL vs NoSQL: Making the Choice

Choose SQL if:

Data relationships are complex (e.g., foreign keys, many-to-many relationships).
You need strong consistency and reliability.
The system involves transactional operations.


Choose NoSQL if:

Scalability and performance are critical.
You need to store large volumes of unstructured/semi-structured data.
The application has rapidly changing requirements and needs a flexible schema.

24. How do you handle errors in a stored procedure?
Error handling in a stored procedure ensures that the procedure can gracefully 
manage exceptions, log errors, or take corrective actions when something goes wrong.
This is typically done using TRY...CATCH blocks in SQL Server or equivalent 
constructs in other database systems.

1. Error Handling Using TRY...CATCH (SQL Server)
The TRY...CATCH block captures errors during execution and allows you to handle 
them in the CATCH block.

BEGIN TRY
    -- Code that might cause an error
END TRY
BEGIN CATCH
    -- Handle the error
END CATCH


2. Functions to Retrieve Error Details
Inside the CATCH block, you can use built-in functions to retrieve detailed error
information:

ERROR_MESSAGE(): Returns the error message.
ERROR_NUMBER(): Returns the error number.
ERROR_SEVERITY(): Returns the error severity level.
ERROR_STATE(): Returns the error state.
ERROR_PROCEDURE(): Returns the name of the procedure where the error occurred.
ERROR_LINE(): Returns the line number where the error occurred.

CREATE PROCEDURE HandleErrorsWithDetails
AS
BEGIN
    BEGIN TRY
        -- Example of an error: inserting into a non-existent table
        INSERT INTO NonExistentTable (Column1) VALUES ('Test');
    END TRY
    BEGIN CATCH
        PRINT 'An error occurred:';
        PRINT 'Message: ' + ERROR_MESSAGE();
        PRINT 'Procedure: ' + ISNULL(ERROR_PROCEDURE(), 'N/A');
        PRINT 'Line: ' + CAST(ERROR_LINE() AS NVARCHAR(10));
    END CATCH
END


3. Rolling Back Transactions on Errors
When a stored procedure involves transactions, ensure you roll back the transaction
in the CATCH block to avoid leaving the database in an inconsistent state.

CREATE PROCEDURE HandleErrorsWithTransaction
AS
BEGIN
    BEGIN TRY
        BEGIN TRANSACTION;
        
        -- Attempt to insert into a table
        INSERT INTO Employees (Id, Name) VALUES (1, 'John Doe');
        
        -- Simulating an error
        INSERT INTO NonExistentTable (Column1) VALUES ('Test');
        
        COMMIT TRANSACTION;
    END TRY
    BEGIN CATCH
        -- Rollback the transaction on error
        IF @@TRANCOUNT > 0
            ROLLBACK TRANSACTION;

        PRINT 'Error occurred. Rolling back transaction.';
        PRINT 'Error: ' + ERROR_MESSAGE();
    END CATCH
END


4. Logging Errors into a Table
For production systems, logging errors into a dedicated error table is a best
practice.

CREATE TABLE ErrorLog (
    ErrorID INT IDENTITY(1,1),
    ErrorNumber INT,
    ErrorMessage NVARCHAR(4000),
    ErrorProcedure NVARCHAR(128),
    ErrorLine INT,
    ErrorDateTime DATETIME DEFAULT GETDATE()
);

CREATE PROCEDURE HandleErrorsWithLogging
AS
BEGIN
    BEGIN TRY
        -- Simulating an error
        INSERT INTO NonExistentTable (Column1) VALUES ('Test');
    END TRY
    BEGIN CATCH
        -- Log error details to the ErrorLog table
        INSERT INTO ErrorLog (ErrorNumber, ErrorMessage, ErrorProcedure, ErrorLine)
        VALUES (
            ERROR_NUMBER(),
            ERROR_MESSAGE(),
            ERROR_PROCEDURE(),
            ERROR_LINE()
        );

        PRINT 'Error occurred and logged to ErrorLog table.';
    END CATCH
END


5. Best Practices for Error Handling in Stored Procedures
1Always Use TRY...CATCH:

Capture and handle unexpected errors gracefully.

2.Roll Back Transactions:

Roll back transactions if errors occur during a transaction to maintain database integrity.

3.Log Errors:

Store error details in a log table for debugging and audit purposes.

4.Avoid PRINT for Critical Errors:

Use logging or return values instead of PRINT for critical error information in production.

5.Return Error Codes or Messages:

Use output parameters or return values to communicate errors to the calling application.

6.Use Specific Error Messages:

Provide meaningful error messages to help identify the root cause.

6. Limitations of TRY...CATCH
Compile-Time Errors: TRY...CATCH cannot handle errors like syntax errors or issues
that occur before execution (e.g., missing tables, invalid column names).
Batch Termination: Certain severe errors (e.g., KILL command, hardware failures) 
may terminate the batch and bypass the CATCH block.

25. Explain the difference between a temp table, table variable, and a CTE.
Temporary Tables, Table Variables, and Common Table Expressions (CTEs) are all ways
to store and manipulate intermediate results in SQL. While they can achieve similar
outcomes, they are used in different scenarios and have distinct characteristics. 
Below is a detailed comparison:

1. Temporary Table
A temporary table is a real table created in the tempdb system database.
It persists for the duration of a session or a transaction, depending on how it is
created.

2. Table Variable
A table variable is a type of variable that stores a result set in memory, primarily 
used for smaller datasets and shorter lifetimes.

3. Common Table Expression (CTE)
A CTE is a temporary, named result set that exists only for the duration of a single
SELECT, INSERT, UPDATE, or DELETE statement.

When to Use Each
Temporary Table:

When working with large datasets or performing multiple operations on the data.
When needing indexes, constraints, or statistics for performance.
For complex scenarios where intermediate results must be stored for reuse.

Table Variable:

For small datasets or when working within a single scope.
When performance overhead from disk I/O is not acceptable.
In scenarios where no advanced indexing or statistics are required.


CTE:

For simple, single-use queries, especially for readability and modularity.
When working with hierarchical or recursive data.
As an alternative to subqueries for better query organization.

26. How do you upTo update data in one table based on values from another table,
you can use the UPDATE statement in SQL combined with a JOIN or a WHERE subquery.
Here are the approaches:

1. Using UPDATE with JOIN
This is the most common and efficient way to update a table using values from 
another table.

UPDATE Table1
SET Table1.Column1 = Table2.Column2
FROM Table1
JOIN Table2
ON Table1.CommonColumn = Table2.CommonColumn
WHERE <Condition>; -- Optional, to filter rows

2. Using UPDATE with a Subquery
This method uses a subquery to fetch values from the other table and update the
target table.

UPDATE Table1
SET Column1 = (SELECT Column2
               FROM Table2
               WHERE Table1.CommonColumn = Table2.CommonColumn)
WHERE EXISTS (SELECT 1
              FROM Table2
              WHERE Table1.CommonColumn = Table2.CommonColumn);


3. Using a CASE Statement in UPDATE
If you need to update based on multiple conditions or values from another table,
you can use a CASE statement.

UPDATE Table1
SET Column1 = CASE 
                WHEN Table2.Column2 = <Condition> THEN <Value>
                ELSE <DefaultValue>
              END
FROM Table1
JOIN Table2
ON Table1.CommonColumn = Table2.CommonColumn;

27. What is the difference between DELETE and TRUNCATE? When would you use each?

The DELETE and TRUNCATE statements in SQL are used to remove rows from a table,
but they differ significantly in terms of functionality, performance, and use cases.


Key Differences Between DELETE and TRUNCATE
Feature			DELETE								TRUNCATE
Operation Type	DML (Data Manipulation Language)	DDL (Data Definition Language)

Row Selection	Can delete specific rows using WHERE.	Removes all rows in the table.

Logging	Logs each deleted row in the transaction log.	Logs only deallocation of data pages.

Triggers	Triggers are fired for each deleted row.	Triggers are not fired.

Rollback	Can be rolled back if wrapped in a transaction.	Can also be rolled back in a transaction.

Identity Reset	Does not reset the identity counter.	Resets the identity counter to its seed value.

Foreign Key Constraints	Enforces foreign key constraints.	Cannot truncate a table with foreign key references.

Performance	Slower for large datasets due to logging and row-by-row deletion.	Faster as it deallocates entire data pages.

Usage	Use to delete specific rows or when foreign keys are involved.	Use to quickly clear all data from a table.


Use DELETE when:

You need to delete specific rows.
Triggers or foreign key constraints are involved.

Use TRUNCATE when:

You need to quickly clear all rows and reset the table.
No foreign key constraints exist on the table.

28. How do you handle NULL values in SQL queries?

Handling NULL values in SQL is important because NULL represents missing or unknown
data and can affect query results if not handled properly. Below are various ways
to manage NULL values in SQL queries:

1. Understanding NULL
NULL is not equal to zero, an empty string, or any other value.
Comparisons involving NULL with any value result in UNKNOWN. 

SELECT 1 WHERE NULL = NULL; 

2. Use IS NULL or IS NOT NULL
To check for NULL values, you cannot use the = operator. Use IS NULL or IS NOT NULL:

SELECT * 
FROM Employees 
WHERE ManagerId IS NULL;

SELECT * 
FROM Employees 
WHERE ManagerId IS NOT NULL;


3. Replace NULL Values Using COALESCE or ISNULL
You can replace NULL values with a default value using these functions:

COALESCE (Standard SQL)
Returns the first non-NULL value in a list of expressions.

SELECT Name, COALESCE(PhoneNumber, 'N/A') AS PhoneNumber
FROM Customers;

4. Handle NULL in Aggregations
Aggregate functions like SUM, AVG, COUNT, etc., ignore NULL values automatically.

SELECT COUNT(*) AS TotalRows, COUNT(Salary) AS NonNullSalaries
FROM Employees;


5. Use Conditional Logic with NULL
Use CASE expressions to handle NULL values in specific scenarios.

SELECT 
    Name, 
    CASE 
        WHEN Salary IS NULL THEN 'No Salary'
        ELSE 'Has Salary'
    END AS SalaryStatus
FROM Employees;


6. Avoid Issues with NULL in Comparisons
Example: Correctly handle NULL in conditions
If you are comparing columns that might contain NULL, use IS NULL or functions 
like COALESCE:

SELECT * 
FROM Orders 
WHERE COALESCE(Discount, 0) > 0;

7. Sorting with NULL
When sorting data, NULL values are treated as the lowest (or highest) values, 
depending on the database. Use ORDER BY with NULLS FIRST or NULLS LAST (if supported).

SELECT * 
FROM Employees 
ORDER BY Salary DESC NULLS LAST;

8. Join Operations Involving NULL
When joining tables, NULL values can cause mismatches. Use IS NULL or COALESCE to
ensure correct results:

SELECT e.Name, d.DepartmentName
FROM Employees e
LEFT JOIN Departments d 
  ON e.DepartmentId = COALESCE(d.DepartmentId, -1);


9. Filter with NULL in Conditional Queries
When filtering based on NULL, you can handle it explicitly in conditions.

SELECT * 
FROM Orders 
WHERE Status = 'Shipped' OR Status IS NULL;

10. Handle NULL in Mathematical or String Operations
Mathematical or string operations with NULL result in NULL. Use COALESCE or 
ISNULL to avoid NULL in results.

SELECT Name, COALESCE(Salary, 0) * 1.1 AS AdjustedSalary
FROM Employees;


29. Write a query to find all employees who have a higher salary than their 
department’s average salary.

To find all employees who have a higher salary than the average salary of their 
respective department, you can use a correlated subquery or a common table 
expression (CTE). Here's the solution using both methods:

1. Using a Correlated Subquery
In this approach, you calculate the average salary for each department using a 
subquery and compare each employee's salary to their department's average salary.

SELECT e.EmployeeId, e.Name, e.DepartmentId, e.Salary
FROM Employees e
WHERE e.Salary > (
    SELECT AVG(e2.Salary)
    FROM Employees e2
    WHERE e2.DepartmentId = e.DepartmentId
);

2. Using a Common Table Expression (CTE)
With a CTE, you first calculate the average salary for each department and then 
join it with the Employees table.

WITH DepartmentAverage AS (
    SELECT DepartmentId, AVG(Salary) AS AvgSalary
    FROM Employees
    GROUP BY DepartmentId
)
SELECT e.EmployeeId, e.Name, e.DepartmentId, e.Salary
FROM Employees e
JOIN DepartmentAverage da
    ON e.DepartmentId = da.DepartmentId
WHERE e.Salary > da.AvgSalary;


SELECT DepartmentId, AVG(Salary) AS AvgSalary
FROM Employees
GROUP BY DepartmentId

30. How can you pivot data in SQL to convert rows into columns?

Pivoting data in SQL is the process of transforming rows into columns. This is
typically done using the PIVOT operator in SQL Server or conditional aggregation in
databases that do not support the PIVOT keyword (e.g., MySQL or PostgreSQL).

SELECT Year, [A] AS ProductA, [B] AS ProductB
FROM (
    SELECT Year, Product, Sales
    FROM Sales
) AS SourceTable
PIVOT (
    SUM(Sales)
    FOR Product IN ([A], [B])
) AS PivotTable;

2. Using Conditional Aggregation (MySQL, PostgreSQL, etc.)
If your database does not support the PIVOT operator, you can achieve the same
result using CASE statements with aggregate functions.

SELECT 
    Year,
    SUM(CASE WHEN Product = 'A' THEN Sales ELSE 0 END) AS ProductA,
    SUM(CASE WHEN Product = 'B' THEN Sales ELSE 0 END) AS ProductB
FROM Sales
GROUP BY Year;


3. Dynamic Pivoting (When Column Values Are Dynamic)
If the Product values are not known in advance, you need to generate the pivot columns dynamically.

Dynamic Pivot in SQL Server:
Here’s an example of dynamically creating pivot columns:

DECLARE @cols NVARCHAR(MAX);
DECLARE @query NVARCHAR(MAX);

-- Get all unique product names to use as columns
SELECT @cols = STRING_AGG(QUOTENAME(Product), ',') 
FROM (SELECT DISTINCT Product FROM Sales) AS Products;

-- Construct the dynamic SQL query
SET @query = '
SELECT Year, ' + @cols + '
FROM (
    SELECT Year, Product, Sales
    FROM Sales
) AS SourceTable
PIVOT (
    SUM(Sales)
    FOR Product IN (' + @cols + ')
) AS PivotTable;';

-- Execute the dynamic query
EXEC sp_executesql @query;


31. Explain self-joins and provide a scenario where they might be useful.
A self-join is a join in which a table is joined with itself. This is achieved
by treating the table as if it were two separate tables by assigning it different 
aliases. Self-joins are typically used when there is a relationship between rows in 
the same table.

SELECT A.Column1, B.Column2
FROM TableName A
JOIN TableName B
ON A.SomeColumn = B.SomeOtherColumn;


32. What is a cross join, and when would you use it?
What is a Cross Join?
A cross join is a type of SQL join that combines every row from one table with every
row from another table. It produces the Cartesian product of the two tables, 
resulting in a dataset that includes all possible combinations of rows.

If Table A has 
𝑚
m rows and Table B has 
𝑛
n rows, the result of a cross join will have 
𝑚
×
𝑛
m×n rows.

A cross join does not require any conditions (like ON or WHERE) to link the rows.

SELECT *
FROM TableA
CROSS JOIN TableB;

33. How would you find the common records between two tables?
To find the common records between two tables, you can use an INNER JOIN 
or the INTERSECT operator, depending on the SQL dialect and requirements.

1. Using INNER JOIN
An INNER JOIN returns only the rows that have matching values in both tables
based on a specified condition.

2. Using INTERSECT (if supported)
The INTERSECT operator returns rows that are present in both result sets of two 
SELECT statements.

Which Method Should You Use?
INNER JOIN: Best when you need to retrieve data from both tables.

INTERSECT: Simplest way to find common rows (if supported by your database).

EXISTS: Efficient for large datasets with complex conditions.

IN: Simple for single-column matching.


34. Write a query to retrieve employees who do not have any subordinates.
To retrieve employees who do not have any subordinates, you can query the Employees 
table by identifying employees whose EmployeeId is not referenced as a ManagerId by 
any other employee.

Which Method to Use?
LEFT JOIN: Readable and commonly used.

NOT EXISTS: Efficient for larger datasets.

NOT IN: Simple for single-column checks but can perform poorly with NULL values.

35. How would you implement many-to-many relationships in a relational database?

In relational databases, a many-to-many relationship occurs when multiple records
in one table can be associated with multiple records in another table. This type
of relationship is implemented using an intersection table (also called a join table,
bridge table, or junction table).

Steps to Implement Many-to-Many Relationships
1.Create Two Primary Tables: Define the two entities that share the 
many-to-many relationship.

2.Create an Intersection Table: Add a third table (junction table) to represent
the relationship. This table will contain foreign keys referencing the primary keys
of the two entities.

3.Add Necessary Constraints: Use primary keys, foreign keys, and optionally unique
constraints to enforce data integrity.

36. How do you design a database to avoid redundancy? Explain with examples.

Designing a Database to Avoid Redundancy
To avoid redundancy in a database, we follow Normalization, which involves 
organizing the data into multiple related tables to reduce data duplication and
ensure data integrity. Normalization is achieved through systematic decomposition 
into smaller tables and defining relationships between them using primary keys and
foreign keys.

Steps to Avoid Redundancy
1.Identify Data Groups: Separate the data into distinct entities or topics.

2.Normalize the Tables: Apply normalization rules (1NF, 2NF, 3NF, etc.).

3.Use Foreign Keys: Define relationships between tables to maintain consistency.

4.Avoid Storing Calculated or Derived Data: Calculate values on the fly, instead 
of storing them in the database.

5.Design Carefully for Repeated Values: Move repeated data to a separate table.

37. What is the purpose of a surrogate key? How does it differ from a natural key?

What is a Surrogate Key?
A surrogate key is a unique identifier for a database record that is not derived 
from any of the record's actual data. It is artificially created by the database, 
typically as an auto-incrementing integer or a UUID (Universally Unique Identifier).

Purpose of a Surrogate Key:
1.Uniqueness: Ensures that every row in a table has a unique identifier.

2.Stability: Surrogate keys remain constant even when the actual data in the record 
changes, which is especially useful in scenarios where natural keys might be prone 
to modification.

3.Simplification: Avoids the complexities and potential risks of using natural keys
(e.g., composite keys).

4.Performance: Numeric surrogate keys (like integers) are small and efficient for 
indexing, improving database performance.

What is a Natural Key?
A natural key is a unique identifier derived from the actual data in the table.
It uses meaningful attributes of the record, such as a social security number (SSN),
email address, or employee code, to uniquely identify rows.


Key Differences Between Surrogate and Natural Keys
Aspect			Surrogate Key						Natural Key
Definition	Artificial, database-generated key.		Derived from actual data attributes.
Meaning		Has no inherent meaning.				Carries business-related meaning.
Stability	Stable, not affected by data changes.	May change if the underlying data changes.
Size		Usually a small integer or UUID.		Can be large or complex (e.g., composite keys).
Simplicity	Simple and efficient.					Can be complex, especially with composite keys.
Risk of Duplication	Zero risk, as it is unique by design.	Risk exists if data validation is not enforced.
Performance	Faster for indexing and lookups.		Slower due to larger size or complexity.
Real-World Example	Auto-incremented OrderID.		Email or SSN as the primary key.

Combined Approach
In many modern designs, databases use both:

1.Surrogate Key: As the primary key for internal operations (e.g., OrderID).

2.Natural Key: As a unique constraint to enforce business logic (e.g., OrderNumber 
must be unique).

This balances the simplicity and efficiency of surrogate keys with the real-world 
relevance of natural keys.

38. What are composite keys, and when would you use them?
What Are Composite Keys?
A composite key is a primary key made up of two or more columns in a table that,
together, uniquely identify a row. These columns, when combined, form a unique 
identifier for each record in the table.

When Would You Use a Composite Key?
You would use a composite key when:

1.No Single Column Can Uniquely Identify Records:

Neither column alone provides uniqueness, but their combination does.
Example: A StudentCourse table with StudentID and CourseID.

2.Capturing Relationships Between Entities:

Composite keys are useful in junction tables for many-to-many relationships.
Example: A ProductOrder table that links ProductID and OrderID.

3.Preserving Business Logic:

If the combination of attributes naturally represents a unique entity in the business domain.
Example: In a bank, a combination of AccountNumber and BranchCode might be unique.

4.Reducing Redundancy:

Using composite keys prevents the need to introduce a surrogate key, avoiding 
redundancy in simple scenarios.

39. Write a query to find consecutive absent days for an employee from an attendance table.

WITH RankedAttendance AS (
    SELECT
        EmployeeID,
        AttendanceDate,
        Status,
        ROW_NUMBER() OVER (PARTITION BY EmployeeID ORDER BY AttendanceDate) -
        DENSE_RANK() OVER (PARTITION BY EmployeeID, Status ORDER BY AttendanceDate) AS GroupID
    FROM
        Attendance
    WHERE
        Status = 'Absent'
),
ConsecutiveAbsences AS (
    SELECT
        EmployeeID,
        MIN(AttendanceDate) AS StartDate,
        MAX(AttendanceDate) AS EndDate,
        COUNT(*) AS TotalAbsentDays
    FROM
        RankedAttendance
    GROUP BY
        EmployeeID, GroupID
)
SELECT
    EmployeeID,
    StartDate,
    EndDate,
    TotalAbsentDays
FROM
    ConsecutiveAbsences;


Explanation
1.RankedAttendance CTE:

ROW_NUMBER(): Assigns a sequential row number to each record for an employee based
on the AttendanceDate.
DENSE_RANK(): Groups records where Status = 'Absent' into distinct ranges based on
the date sequence.
The difference between the two generates a GroupID, which identifies consecutive 
"islands" of absence.

2.ConsecutiveAbsences CTE:

Use the GroupID to group consecutive absent days for each employee.

MIN(AttendanceDate) gives the start date of the absence range.

MAX(AttendanceDate) gives the end date of the absence range.

COUNT(*) gives the total number of days in the absence range.

3.Final Query:

Select the EmployeeID, StartDate, EndDate, and TotalAbsentDays from the grouped data.


Alternative for Simpler Scenarios
If your database doesn't support window functions, you can achieve a similar result
by iterating through records with self-joins or temporary tables. However, 
window functions are highly efficient and recommended for this type of analysis.
Let me know if you'd like an alternative query!

40. How would you merge data from two tables that have different schemas?
Merging data from two tables with different schemas requires carefully mapping
or aligning the data fields from both tables. The strategy depends on the specific
requirements, such as whether you want to combine data into a single table, view,
or result set, and how the columns from both tables relate to each other.

Approach to Merging Data
1. Use UNION for Similar Columns
If the tables have some common columns but also different ones, you can use a
UNION query to combine the shared columns while ignoring the others.


2. Join Tables When There’s a Relationship
If the tables have related fields (e.g., primary/foreign keys), use JOIN to merge
the data.
This approach is useful when you want to combine related data while keeping all 
columns from both tables.

41. Design a query to calculate the retention rate of users over time.

To calculate the retention rate of users over time, you need to track the activity
or usage of users over specific time periods (e.g., weekly, monthly). Retention rate
typically measures the percentage of users who return after a specific time period,
such as the number of users who were active in a given week or month and came back
in subsequent periods.

Explanation

1.UserActivityByMonth CTE:

This step extracts the year and month from the activity date for each user. 
It groups the activity by user, year, and month.

2.FirstMonthActive CTE:

This identifies the first month that each user was active, starting from their 
signup month.
It joins the Users table with UserActivity to capture the first activity date for
each user.

3.Retention CTE:

This counts how many users were active in both the signup month and the following
month. The following month is considered for retention.

4.RetentionRate Calculation:

The LAG() function is used to retrieve the number of active users from the previous
month, and then the retention rate is calculated based on the active users in the 
current month divided by the previous month's active users.

The result gives the Retention Rate as a percentage.

42. Write a query to identify gaps in a sequence of numbers in a column.

To identify gaps in a sequence of numbers in a column, you can use a query that
checks for missing numbers between consecutive values in that column. This type
of query typically involves comparing each row with the next row to find missing 
numbers in the sequence.

Explanation
1.LEAD Function: The LEAD() function is used to get the value of the next row in the
sequence (based on the ORDER BY clause). It allows us to compare each number with
the next number in the sequence.


2.CTE (Numbered): The common table expression (CTE) Numbered fetches the Number and
the next number (NextNumber) for each row.

3.Gaps Identification: In the main query, we check if the difference between the
current number (Number) and the next number (NextNumber) is greater than 1. 
If the difference is greater than 1, it means there is a gap in the sequence.
We then return the first missing number in the gap, which is Number + 1.

